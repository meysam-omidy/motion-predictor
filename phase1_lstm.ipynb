{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "75a4c314",
      "metadata": {},
      "source": [
        "# Phase 1: LSTM motion predictor training\n",
        "\n",
        "Train the improved LSTM predictor on the same datasets and setup as the transformer (phase1_improved). Uses `GTSequenceDataset` (src, trg, gt_src, gt_trg) and `LossFunction` with CIoU + confidence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "48627a07",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 372563, Val samples: 221031\n"
          ]
        }
      ],
      "source": [
        "from dataset import GTSequenceDataset\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "\n",
        "SEQ_IN_LEN = 30\n",
        "SEQ_OUT_LEN = 20\n",
        "SEQ_TOTAL_LEN = 50\n",
        "BATCH_SIZE = 512\n",
        "STEPS = 4\n",
        "NOISE_COEFFICIENT = 0.15\n",
        "NOISE_PROB = 0\n",
        "\n",
        "BASE_DIR = '../../Datasets/'\n",
        "train_dataset = GTSequenceDataset.from_roots([\n",
        "    f'{BASE_DIR}/SportsMOT/train',\n",
        "    f'{BASE_DIR}DanceTrack/train',\n",
        "    f'{BASE_DIR}MOT17/train',\n",
        "    f'{BASE_DIR}MOT20/train'\n",
        "], seq_in_len=SEQ_IN_LEN, seq_out_len=SEQ_OUT_LEN, seq_total_len=SEQ_TOTAL_LEN, steps=STEPS, noise_coeff=NOISE_COEFFICIENT, noise_prob=NOISE_PROB)\n",
        "\n",
        "val_dataset = GTSequenceDataset.from_roots([\n",
        "    f'{BASE_DIR}/SportsMOT/val',\n",
        "    f'{BASE_DIR}DanceTrack/val',\n",
        "    f'{BASE_DIR}MOT17/val',\n",
        "    f'{BASE_DIR}MOT20/val'\n",
        "], seq_in_len=SEQ_IN_LEN, seq_out_len=SEQ_OUT_LEN, seq_total_len=SEQ_TOTAL_LEN, steps=STEPS, noise_coeff=NOISE_COEFFICIENT, noise_prob=NOISE_PROB)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "print(f'Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7083a6c1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.Size([512, 30, 13]), torch.Size([512, 20, 13]), torch.Size([512, 30, 13]), torch.Size([512, 20, 13])]\n"
          ]
        }
      ],
      "source": [
        "# Optional: inspect one batch (src, trg, gt_src, gt_trg)\n",
        "d = next(iter(train_loader))\n",
        "print([x.shape for x in d])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0682ccf5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 6,763,525\n"
          ]
        }
      ],
      "source": [
        "from lstm_improved import ImprovedLSTMPredictor\n",
        "from loss import LossFunction\n",
        "from torch import optim\n",
        "\n",
        "DEVICE = 'cuda'\n",
        "model = ImprovedLSTMPredictor(\n",
        "    input_dim=13,\n",
        "    output_dim=5,\n",
        "    d_model=512,\n",
        "    hidden_dim=1024,\n",
        "    num_layers=1,\n",
        "    dropout=0,\n",
        "    teacher_forcing_ratio=1,\n",
        ").to(DEVICE)\n",
        "criterion = LossFunction(loss1_coeff=1, loss2_coeff=0, loss3_coeff=0, loss4_coeff=0)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e071a4b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: load a pretrained LSTM checkpoint\n",
        "model.load_weight('pretrained/lstm-new-d512-h1024.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e42a74ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.evaluate(val_loader, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a3fcaf04",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss = 1.01493908, Val Loss = 0.98717827, LR = 0.00049872\n",
            "Epoch 2: Train Loss = 0.28754904, Val Loss = 0.11128036, LR = 0.00049488\n",
            "Epoch 3: Train Loss = 0.13727466, Val Loss = 0.09865989, LR = 0.00048853\n",
            "Epoch 4: Train Loss = 0.11444886, Val Loss = 0.09831966, LR = 0.00047974\n",
            "Epoch 5: Train Loss = 0.10489155, Val Loss = 0.10434487, LR = 0.00046859\n",
            "Epoch 6: Train Loss = 0.09625999, Val Loss = 0.09516920, LR = 0.00045519\n",
            "Epoch 7: Train Loss = 0.09020396, Val Loss = 0.09390839, LR = 0.00043969\n",
            "Epoch 8: Train Loss = 0.09116861, Val Loss = 0.09483480, LR = 0.00042224\n",
            "Epoch 9: Train Loss = 0.08809761, Val Loss = 0.09624782, LR = 0.00040303\n",
            "Epoch 10: Train Loss = 0.08433444, Val Loss = 0.09237980, LR = 0.00038224\n",
            "Epoch 11: Train Loss = 0.08076018, Val Loss = 0.09301889, LR = 0.00036010\n",
            "Epoch 12: Train Loss = 0.07463898, Val Loss = 0.06794308, LR = 0.00033683\n",
            "Epoch 13: Train Loss = 0.06083539, Val Loss = 0.06693332, LR = 0.00031266\n",
            "Epoch 14: Train Loss = 0.05818817, Val Loss = 0.06397889, LR = 0.00028786\n",
            "Epoch 15: Train Loss = 0.05498580, Val Loss = 0.06085269, LR = 0.00026266\n",
            "Epoch 16: Train Loss = 0.05194247, Val Loss = 0.05813880, LR = 0.00023734\n",
            "Epoch 17: Train Loss = 0.05053213, Val Loss = 0.05746009, LR = 0.00021214\n",
            "Epoch 18: Train Loss = 0.04877688, Val Loss = 0.05694213, LR = 0.00018734\n",
            "Epoch 19: Train Loss = 0.04825190, Val Loss = 0.05538977, LR = 0.00016317\n",
            "Epoch 20: Train Loss = 0.04762913, Val Loss = 0.05545488, LR = 0.00013990\n",
            "Epoch 21: Train Loss = 0.04694665, Val Loss = 0.05479122, LR = 0.00011776\n",
            "Epoch 22: Train Loss = 0.04625491, Val Loss = 0.05451123, LR = 0.00009697\n",
            "Epoch 23: Train Loss = 0.04603532, Val Loss = 0.05439968, LR = 0.00007776\n",
            "Epoch 24: Train Loss = 0.04573230, Val Loss = 0.05454951, LR = 0.00006031\n",
            "Epoch 25: Train Loss = 0.04536655, Val Loss = 0.05469430, LR = 0.00004481\n",
            "Epoch 26: Train Loss = 0.04515116, Val Loss = 0.05448265, LR = 0.00003141\n",
            "Epoch 27: Train Loss = 0.04494125, Val Loss = 0.05480668, LR = 0.00002026\n",
            "Epoch 28: Train Loss = 0.04479508, Val Loss = 0.05431938, LR = 0.00001147\n",
            "Epoch 29: Train Loss = 0.04471110, Val Loss = 0.05465907, LR = 0.00000512\n",
            "Epoch 30: Train Loss = 0.04464578, Val Loss = 0.05457166, LR = 0.00000128\n",
            "Training complete. Best Val Loss: 0.054319378212784174\n"
          ]
        }
      ],
      "source": [
        "LR = 5e-4\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    betas=(0.9, 0.999),\n",
        "    weight_decay=1e-4,\n",
        "    eps=1e-8\n",
        ")\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS + 1)\n",
        "best_val_loss = float(\"inf\")\n",
        "log_file = open('file_lstm.log', 'w')\n",
        "log_file.close()\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    train_loss = model.train_one_epoch(train_loader, optimizer, criterion, device=DEVICE)\n",
        "    val_loss = model.evaluate(val_loader, criterion, device=DEVICE)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        model.save_weight('pretrained/lstm-new-d256-h256-e3-d3.pth')\n",
        "\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.8f}, Val Loss = {val_loss:.8f}, LR = {current_lr:.8f}\")\n",
        "    log_file = open('file_lstm.log', 'a')\n",
        "    log_file.write(f\"Epoch {epoch}: Train Loss = {train_loss:.8f}, Val Loss = {val_loss:.8f}, LR = {current_lr:.8f}\\n\")\n",
        "    log_file.close()\n",
        "\n",
        "print(\"Training complete. Best Val Loss:\", best_val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "56a43070",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_weight('pretrained/lstm-new-d512-h1024.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c4e9cae7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 6,763,525\n",
            "Epoch 1: Train Loss = 0.61372989, Val Loss = 0.27502551, LR = 0.00049972\n",
            "Epoch 2: Train Loss = 0.27050042, Val Loss = 0.17924333, LR = 0.00049887\n",
            "Epoch 3: Train Loss = 0.21027387, Val Loss = 0.14968078, LR = 0.00049746\n",
            "Epoch 4: Train Loss = 0.19658001, Val Loss = 0.16729819, LR = 0.00049548\n",
            "Epoch 5: Train Loss = 0.15750353, Val Loss = 0.12727606, LR = 0.00049295\n",
            "Epoch 6: Train Loss = 0.13628713, Val Loss = 0.11548214, LR = 0.00048987\n",
            "Epoch 7: Train Loss = 0.12436708, Val Loss = 0.10598248, LR = 0.00048625\n",
            "Epoch 8: Train Loss = 0.12670715, Val Loss = 0.13624554, LR = 0.00048209\n",
            "Epoch 9: Train Loss = 0.12757746, Val Loss = 0.09944880, LR = 0.00047741\n",
            "Epoch 10: Train Loss = 0.10750821, Val Loss = 0.09436887, LR = 0.00047221\n",
            "Epoch 11: Train Loss = 0.09445427, Val Loss = 0.09535331, LR = 0.00046651\n",
            "Epoch 12: Train Loss = 0.09057219, Val Loss = 0.09372558, LR = 0.00046031\n",
            "Epoch 13: Train Loss = 0.09011029, Val Loss = 0.09613086, LR = 0.00045364\n",
            "Epoch 14: Train Loss = 0.09049712, Val Loss = 0.10093074, LR = 0.00044651\n",
            "Epoch 15: Train Loss = 0.08473179, Val Loss = 0.09502122, LR = 0.00043894\n",
            "Epoch 16: Train Loss = 0.07388912, Val Loss = 0.06908488, LR = 0.00043093\n",
            "Epoch 17: Train Loss = 0.06341670, Val Loss = 0.07116563, LR = 0.00042252\n",
            "Epoch 18: Train Loss = 0.05998272, Val Loss = 0.06874561, LR = 0.00041372\n",
            "Epoch 19: Train Loss = 0.05753064, Val Loss = 0.06456736, LR = 0.00040454\n",
            "Epoch 20: Train Loss = 0.05672221, Val Loss = 0.06260862, LR = 0.00039501\n",
            "Epoch 21: Train Loss = 0.05487413, Val Loss = 0.06271701, LR = 0.00038516\n",
            "Epoch 22: Train Loss = 0.05297915, Val Loss = 0.05920860, LR = 0.00037500\n",
            "Epoch 23: Train Loss = 0.05263882, Val Loss = 0.05767497, LR = 0.00036456\n",
            "Epoch 24: Train Loss = 0.04921562, Val Loss = 0.05666477, LR = 0.00035385\n",
            "Epoch 25: Train Loss = 0.04873345, Val Loss = 0.05483331, LR = 0.00034292\n",
            "Epoch 26: Train Loss = 0.04623588, Val Loss = 0.05450813, LR = 0.00033177\n",
            "Epoch 27: Train Loss = 0.04805100, Val Loss = 0.05532491, LR = 0.00032043\n",
            "Epoch 28: Train Loss = 0.04498832, Val Loss = 0.05324941, LR = 0.00030894\n",
            "Epoch 29: Train Loss = 0.04551027, Val Loss = 0.05286818, LR = 0.00029731\n",
            "Epoch 30: Train Loss = 0.04376974, Val Loss = 0.05283957, LR = 0.00028558\n",
            "Training complete. Best Val Loss: 0.052839571368639114\n",
            "Epoch 1: Train Loss = 0.07563649, Val Loss = 0.05830492, LR = 0.00027376\n",
            "Epoch 2: Train Loss = 0.05175653, Val Loss = 0.04741941, LR = 0.00026190\n",
            "Epoch 3: Train Loss = 0.04642140, Val Loss = 0.04330149, LR = 0.00025000\n",
            "Epoch 4: Train Loss = 0.04157820, Val Loss = 0.04121420, LR = 0.00023810\n",
            "Epoch 5: Train Loss = 0.04003495, Val Loss = 0.03995774, LR = 0.00022624\n",
            "Epoch 6: Train Loss = 0.03886759, Val Loss = 0.03901166, LR = 0.00021442\n",
            "Epoch 7: Train Loss = 0.03807143, Val Loss = 0.03876788, LR = 0.00020269\n",
            "Epoch 8: Train Loss = 0.03799602, Val Loss = 0.03774905, LR = 0.00019106\n",
            "Epoch 9: Train Loss = 0.03626496, Val Loss = 0.03752532, LR = 0.00017957\n",
            "Epoch 10: Train Loss = 0.03576559, Val Loss = 0.03693937, LR = 0.00016823\n",
            "Epoch 11: Train Loss = 0.03530448, Val Loss = 0.03658829, LR = 0.00015708\n",
            "Epoch 12: Train Loss = 0.03547083, Val Loss = 0.03804777, LR = 0.00014615\n",
            "Epoch 13: Train Loss = 0.03447698, Val Loss = 0.03614485, LR = 0.00013544\n",
            "Epoch 14: Train Loss = 0.03412187, Val Loss = 0.03576755, LR = 0.00012500\n",
            "Epoch 15: Train Loss = 0.03387433, Val Loss = 0.03560476, LR = 0.00011484\n",
            "Training complete. Best Val Loss: 0.035604763965777775\n",
            "Epoch 1: Train Loss = 0.05033891, Val Loss = 0.04862403, LR = 0.00010499\n",
            "Epoch 2: Train Loss = 0.04807839, Val Loss = 0.04698621, LR = 0.00009546\n",
            "Epoch 3: Train Loss = 0.04731698, Val Loss = 0.04634668, LR = 0.00008628\n",
            "Epoch 4: Train Loss = 0.04708380, Val Loss = 0.04598405, LR = 0.00007748\n",
            "Epoch 5: Train Loss = 0.04656977, Val Loss = 0.04568004, LR = 0.00006907\n",
            "Epoch 6: Train Loss = 0.04617518, Val Loss = 0.04545042, LR = 0.00006106\n",
            "Epoch 7: Train Loss = 0.04586833, Val Loss = 0.04519066, LR = 0.00005349\n",
            "Epoch 8: Train Loss = 0.04554124, Val Loss = 0.04506894, LR = 0.00004636\n",
            "Epoch 9: Train Loss = 0.04525293, Val Loss = 0.04489610, LR = 0.00003969\n",
            "Epoch 10: Train Loss = 0.04503702, Val Loss = 0.04463984, LR = 0.00003349\n",
            "Training complete. Best Val Loss: 0.044639839687281184\n",
            "Epoch 1: Train Loss = 0.06073463, Val Loss = 0.05783500, LR = 0.00002779\n",
            "Epoch 2: Train Loss = 0.06006699, Val Loss = 0.05728152, LR = 0.00002259\n",
            "Epoch 3: Train Loss = 0.05969465, Val Loss = 0.05720092, LR = 0.00001791\n",
            "Epoch 4: Train Loss = 0.05945181, Val Loss = 0.05718688, LR = 0.00001375\n",
            "Epoch 5: Train Loss = 0.05926011, Val Loss = 0.05673729, LR = 0.00001013\n",
            "Epoch 6: Train Loss = 0.05913330, Val Loss = 0.05687265, LR = 0.00000705\n",
            "Epoch 7: Train Loss = 0.05903816, Val Loss = 0.05678414, LR = 0.00000452\n",
            "Epoch 8: Train Loss = 0.05897245, Val Loss = 0.05673888, LR = 0.00000254\n",
            "Epoch 9: Train Loss = 0.05893154, Val Loss = 0.05670982, LR = 0.00000113\n",
            "Epoch 10: Train Loss = 0.05888820, Val Loss = 0.05668560, LR = 0.00000028\n",
            "Training complete. Best Val Loss: 0.056685597032170604\n"
          ]
        }
      ],
      "source": [
        "from dataset import GTSequenceDataset\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from transformer_encoder import MotionTransformer\n",
        "from loss import LossFunction\n",
        "from torch import optim\n",
        "import math, torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import os\n",
        "\n",
        "\n",
        "SEQ_IN_LEN = 30\n",
        "SEQ_OUT_LEN = 20\n",
        "SEQ_TOTAL_LEN = 50\n",
        "BATCH_SIZE = 1024\n",
        "STEPS = 4\n",
        "NOISE_PROB = 0\n",
        "NOISE_COEFFICIENT = 0.15\n",
        "LR = 5e-4\n",
        "DEVICE = 'cuda'\n",
        "PARAMS = [[30, 0], [15, 0.2], [10, 0.4], [10, 0.6]]\n",
        "\n",
        "DEVICE = 'cuda'\n",
        "model = ImprovedLSTMPredictor(\n",
        "    input_dim=13,\n",
        "    output_dim=5,\n",
        "    d_model=512,\n",
        "    hidden_dim=1024,\n",
        "    num_layers=1,\n",
        "    dropout=0.15,\n",
        "    teacher_forcing_ratio=1,\n",
        ").to(DEVICE)\n",
        "criterion = LossFunction(loss1_coeff=1, loss2_coeff=0, loss3_coeff=0, loss4_coeff=0)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    betas=(0.9, 0.999),\n",
        "    weight_decay=1e-4,\n",
        "    eps=1e-8\n",
        ")\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=sum([p[0] for p in PARAMS]) + 1)\n",
        "log_file = open('file_lstm.log', 'w')\n",
        "log_file.close()\n",
        "\n",
        "for param in PARAMS:\n",
        "    NUM_EPOCHS, NOISE_PROB = param\n",
        "    if NOISE_PROB > 0:\n",
        "        criterion = LossFunction(loss1_coeff=0.5, loss2_coeff=0, loss3_coeff=1, loss4_coeff=0)\n",
        "    else:\n",
        "        criterion = LossFunction(loss1_coeff=1, loss2_coeff=0, loss3_coeff=0, loss4_coeff=0)\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    BASE_DIR = '../../Datasets/'\n",
        "    train_dataset = GTSequenceDataset.from_roots([\n",
        "        f'{BASE_DIR}/SportsMOT/train',\n",
        "        f'{BASE_DIR}DanceTrack/train',\n",
        "        f'{BASE_DIR}MOT17/train',\n",
        "        f'{BASE_DIR}MOT20/train'\n",
        "    ], seq_in_len=SEQ_IN_LEN, seq_out_len=SEQ_OUT_LEN, seq_total_len=SEQ_TOTAL_LEN, steps=STEPS, noise_coeff=NOISE_COEFFICIENT, noise_prob=NOISE_PROB)\n",
        "    val_dataset = GTSequenceDataset.from_roots([\n",
        "        f'{BASE_DIR}/SportsMOT/val',\n",
        "        f'{BASE_DIR}DanceTrack/val',\n",
        "        f'{BASE_DIR}MOT17/val',\n",
        "        f'{BASE_DIR}MOT20/val'\n",
        "    # ], seq_in_len=SEQ_IN_LEN, seq_out_len=SEQ_OUT_LEN, seq_total_len=SEQ_TOTAL_LEN, steps=STEPS, noise_coeff=0, noise_prob=0)\n",
        "    ], seq_in_len=SEQ_IN_LEN, seq_out_len=SEQ_OUT_LEN, seq_total_len=SEQ_TOTAL_LEN, steps=STEPS, noise_coeff=NOISE_COEFFICIENT, noise_prob=NOISE_PROB)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        train_loss = model.train_one_epoch(train_loader, optimizer, criterion)\n",
        "        val_loss = model.evaluate(val_loader, criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            model.save_weight('pretrained/lstm-new-d512-h1024.pth')\n",
        "\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.8f}, Val Loss = {val_loss:.8f}, LR = {current_lr:.8f}\")\n",
        "        log_file = open('file_lstm.log', 'a')\n",
        "        log_file.write(f\"Epoch {epoch}: Train Loss = {train_loss:.8f}, Val Loss = {val_loss:.8f}, LR = {current_lr:.8f}\\n\")\n",
        "        log_file.close()\n",
        "\n",
        "    print(\"Training complete. Best Val Loss:\", best_val_loss)\n",
        "\n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0d2f6a8f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.8807, device='cuda:0', grad_fn=<MeanBackward0>)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from dataset import GTSequenceDataset\n",
        "\n",
        "# SEQ_PATH = '../../Datasets/MOT20/val/MOT20-01/'\n",
        "SEQ_PATH = '../../Datasets/MOT17/val/MOT17-05-FRCNN//'\n",
        "# SEQ_PATH = '../../Datasets/DanceTrack/val/dancetrack0018/'\n",
        "# SEQ_PATH = '../../Datasets/DanceTrack/train/dancetrack0006//'\n",
        "# SEQ_PATH = '../../Datasets/SportsMOT/val/v_0kUtTtmLaJA_c004//'\n",
        "SEQ_IN_LEN = 5\n",
        "SEQ_OUT_LEN = 2\n",
        "SEQ_TOTAL_LEN = SEQ_IN_LEN + SEQ_OUT_LEN\n",
        "BATCH_SIZE = 512\n",
        "STEPS = 4\n",
        "\n",
        "# d = GTSequenceDataset.from_sequence(SEQ_PATH, seq_in_len=SEQ_IN_LEN, seq_out_len=SEQ_OUT_LEN, seq_total_len=SEQ_TOTAL_LEN, noise_prob=0, noise_coeff=0)\n",
        "d = GTSequenceDataset.from_sequence(SEQ_PATH, seq_in_len=SEQ_IN_LEN, seq_out_len=SEQ_OUT_LEN, seq_total_len=SEQ_TOTAL_LEN, steps=STEPS, noise_coeff=0.15, noise_prob=0.4)\n",
        "sources = torch.tensor(d.sources).to(DEVICE)[:BATCH_SIZE]\n",
        "# targets = torch.tensor(d.targets).to(DEVICE)[:BATCH_SIZE]\n",
        "gt_sources = torch.tensor(d.gt_sources).to(DEVICE)[:BATCH_SIZE]\n",
        "targets = torch.tensor(d.gt_targets).to(DEVICE)[:BATCH_SIZE]\n",
        "# gt_targets = torch.tensor(d.gt_targets).to(DEVICE)[:BATCH_SIZE]\n",
        "\n",
        "# o = model.inference(sources, targets, num_steps=targets.size(1) - 1)\n",
        "o = model.forward(sources, targets[:, :-1])\n",
        "\n",
        "o[:, :, 0] *= d.image_width.item()\n",
        "o[:, :, 2] *= d.image_width.item()\n",
        "o[:, :, 1] *= d.image_height.item()\n",
        "o[:, :, 3] *= d.image_height.item()\n",
        "targets[:, :, 0] *= d.image_width.item()\n",
        "targets[:, :, 2] *= d.image_width.item()\n",
        "targets[:, :, 1] *= d.image_height.item()\n",
        "targets[:, :, 3] *= d.image_height.item()\n",
        "t_ = targets[:, 1:, :4]\n",
        "sources[:, :, 0] *= d.image_width.item()\n",
        "sources[:, :, 2] *= d.image_width.item()\n",
        "sources[:, :, 1] *= d.image_height.item()\n",
        "sources[:, :, 3] *= d.image_height.item()\n",
        "o_ = o[:, :, :4]\n",
        "\n",
        "index = 9\n",
        "# t[index], o[index]\n",
        "# f = (t[index] - o[index]).abs()[:, 3].mean()\n",
        "# torch.diagonal(criterion.iou(t, o), dim1=1, dim2=2).mean()\n",
        "# criterion.iou(t_, o_).mean()\n",
        "criterion.ciou(t_, o_).mean()\n",
        "# f = (t_ - o_).abs().mean()\n",
        "# f\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
