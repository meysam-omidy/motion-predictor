{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "75a4c314",
      "metadata": {},
      "source": [
        "# Phase 1: LSTM motion predictor training\n",
        "\n",
        "Train the improved LSTM predictor on the same datasets and setup as the transformer (phase1_improved). Uses `GTSequenceDataset` (src, trg, gt_src, gt_trg) and `LossFunction` with CIoU + confidence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "48627a07",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 15072, Val samples: 28163\n"
          ]
        }
      ],
      "source": [
        "from dataset import GTSequenceDataset\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "\n",
        "SEQ_IN_LEN = 30\n",
        "SEQ_OUT_LEN = 20\n",
        "SEQ_TOTAL_LEN = 50\n",
        "BATCH_SIZE = 512\n",
        "STEPS = 4\n",
        "NOISE_COEFFICIENT = 0.15\n",
        "NOISE_PROB = 0\n",
        "\n",
        "BASE_DIR = '../../Datasets/'\n",
        "train_dataset = GTSequenceDataset.from_roots([\n",
        "    # f'{BASE_DIR}/SportsMOT/train',\n",
        "    # f'{BASE_DIR}DanceTrack/train',\n",
        "    f'{BASE_DIR}MOT17/train',\n",
        "    # f'{BASE_DIR}MOT20/train'\n",
        "], seq_in_len=SEQ_IN_LEN, seq_out_len=SEQ_OUT_LEN, seq_total_len=SEQ_TOTAL_LEN, steps=STEPS, noise_coeff=NOISE_COEFFICIENT, noise_prob=NOISE_PROB)\n",
        "\n",
        "val_dataset = GTSequenceDataset.from_roots([\n",
        "    # f'{BASE_DIR}/SportsMOT/val',\n",
        "    # f'{BASE_DIR}DanceTrack/val',\n",
        "    f'{BASE_DIR}MOT17/val',\n",
        "    # f'{BASE_DIR}MOT20/val'\n",
        "], seq_in_len=SEQ_IN_LEN, seq_out_len=SEQ_OUT_LEN, seq_total_len=SEQ_TOTAL_LEN, steps=STEPS, noise_coeff=NOISE_COEFFICIENT, noise_prob=NOISE_PROB)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "print(f'Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7083a6c1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[torch.Size([512, 30, 13]), torch.Size([512, 20, 13]), torch.Size([512, 30, 13]), torch.Size([512, 20, 13])]\n"
          ]
        }
      ],
      "source": [
        "# Optional: inspect one batch (src, trg, gt_src, gt_trg)\n",
        "d = next(iter(train_loader))\n",
        "print([x.shape for x in d])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0682ccf5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 4,161,285\n"
          ]
        }
      ],
      "source": [
        "from lstm_improved import ImprovedLSTMPredictor\n",
        "from loss import LossFunction\n",
        "from torch import optim\n",
        "\n",
        "DEVICE = 'cuda'\n",
        "model = ImprovedLSTMPredictor(\n",
        "    input_dim=13,\n",
        "    output_dim=5,\n",
        "    d_model=256,\n",
        "    hidden_dim=256,\n",
        "    num_encoder_layers=2,\n",
        "    num_decoder_layers=2,\n",
        "    dropout=0.1,\n",
        "    teacher_forcing_ratio=0,\n",
        ").to(DEVICE)\n",
        "criterion = LossFunction(loss1_coeff=1, loss2_coeff=0, loss3_coeff=0, loss4_coeff=0)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e071a4b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: load a pretrained LSTM checkpoint\n",
        "# model.load_weight('pretrained/lstm-improved.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e42a74ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.evaluate(val_loader, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a3fcaf04",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss = 7.01318421, Val Loss = 1.01952448, LR = 0.00198883\n",
            "Epoch 2: Train Loss = 1.01882707, Val Loss = 1.01341448, LR = 0.00195557\n",
            "Epoch 3: Train Loss = 1.01424137, Val Loss = 1.01113686, LR = 0.00190097\n",
            "Epoch 4: Train Loss = 1.01189617, Val Loss = 1.00974627, LR = 0.00182624\n",
            "Epoch 5: Train Loss = 1.01079286, Val Loss = 1.00924521, LR = 0.00173305\n",
            "Epoch 6: Train Loss = 1.01006562, Val Loss = 1.00890190, LR = 0.00162349\n",
            "Epoch 7: Train Loss = 1.00967155, Val Loss = 1.00819183, LR = 0.00150000\n",
            "Epoch 8: Train Loss = 1.00912340, Val Loss = 1.00828261, LR = 0.00136534\n",
            "Epoch 9: Train Loss = 1.00894823, Val Loss = 1.00822236, LR = 0.00122252\n",
            "Epoch 10: Train Loss = 1.00873700, Val Loss = 1.00812816, LR = 0.00107473\n",
            "Epoch 11: Train Loss = 1.00848333, Val Loss = 1.00770973, LR = 0.00092527\n",
            "Epoch 12: Train Loss = 1.00829817, Val Loss = 1.00759045, LR = 0.00077748\n",
            "Epoch 13: Train Loss = 1.00823109, Val Loss = 1.00752290, LR = 0.00063466\n",
            "Epoch 14: Train Loss = 1.00817100, Val Loss = 1.00733981, LR = 0.00050000\n",
            "Epoch 15: Train Loss = 1.00803541, Val Loss = 1.00725962, LR = 0.00037651\n",
            "Epoch 16: Train Loss = 1.00797822, Val Loss = 1.00705557, LR = 0.00026695\n",
            "Epoch 17: Train Loss = 1.00793511, Val Loss = 1.00761666, LR = 0.00017376\n",
            "Epoch 18: Train Loss = 1.00784918, Val Loss = 1.00728168, LR = 0.00009903\n",
            "Epoch 19: Train Loss = 1.00786872, Val Loss = 1.00702197, LR = 0.00004443\n",
            "Epoch 20: Train Loss = 1.00779420, Val Loss = 1.00697900, LR = 0.00001117\n",
            "Training complete. Best Val Loss: 1.0069790035486221\n"
          ]
        }
      ],
      "source": [
        "LR = 2e-3\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    betas=(0.9, 0.999),\n",
        "    weight_decay=1e-4,\n",
        "    eps=1e-8\n",
        ")\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS + 1)\n",
        "best_val_loss = float(\"inf\")\n",
        "log_file = open('file_lstm.log', 'w')\n",
        "log_file.close()\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    train_loss = model.train_one_epoch(train_loader, optimizer, criterion, device=DEVICE)\n",
        "    val_loss = model.evaluate(val_loader, criterion, device=DEVICE)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        model.save_weight('pretrained/lstm-new-d256-h256-e3-d3.pth')\n",
        "\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.8f}, Val Loss = {val_loss:.8f}, LR = {current_lr:.8f}\")\n",
        "    log_file = open('file_lstm.log', 'a')\n",
        "    log_file.write(f\"Epoch {epoch}: Train Loss = {train_loss:.8f}, Val Loss = {val_loss:.8f}, LR = {current_lr:.8f}\\n\")\n",
        "    log_file.close()\n",
        "\n",
        "print(\"Training complete. Best Val Loss:\", best_val_loss)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
