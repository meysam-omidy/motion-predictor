{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "75a4c314",
      "metadata": {},
      "source": [
        "# Phase 1: LSTM motion predictor training\n",
        "\n",
        "Train the improved LSTM predictor on the same datasets and setup as the transformer (phase1_improved). Uses `GTSequenceDataset` (src, trg, gt_src, gt_trg) and `LossFunction` with CIoU + confidence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48627a07",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataset import GTSequenceDataset\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "\n",
        "SEQ_IN_LEN = 30\n",
        "SEQ_OUT_LEN = 20\n",
        "SEQ_TOTAL_LEN = 50\n",
        "BATCH_SIZE = 512\n",
        "STEPS = 4\n",
        "NOISE_COEFFICIENT = 0.15\n",
        "NOISE_PROB = 0.4\n",
        "\n",
        "BASE_DIR = '../../Datasets/'\n",
        "train_dataset = GTSequenceDataset.from_roots([\n",
        "    f'{BASE_DIR}/SportsMOT/train',\n",
        "    f'{BASE_DIR}DanceTrack/train',\n",
        "    f'{BASE_DIR}MOT17/train',\n",
        "    f'{BASE_DIR}MOT20/train'\n",
        "], seq_in_len=SEQ_IN_LEN, seq_out_len=SEQ_OUT_LEN, seq_total_len=SEQ_TOTAL_LEN, steps=STEPS, noise_coeff=NOISE_COEFFICIENT, noise_prob=NOISE_PROB)\n",
        "\n",
        "val_dataset = GTSequenceDataset.from_roots([\n",
        "    f'{BASE_DIR}/SportsMOT/val',\n",
        "    f'{BASE_DIR}DanceTrack/val',\n",
        "    f'{BASE_DIR}MOT17/val',\n",
        "    f'{BASE_DIR}MOT20/val'\n",
        "], seq_in_len=SEQ_IN_LEN, seq_out_len=SEQ_OUT_LEN, seq_total_len=SEQ_TOTAL_LEN, steps=STEPS, noise_coeff=NOISE_COEFFICIENT, noise_prob=NOISE_PROB)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "print(f'Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7083a6c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: inspect one batch (src, trg, gt_src, gt_trg)\n",
        "d = next(iter(train_loader))\n",
        "print([x.shape for x in d])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0682ccf5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from lstm_improved import ImprovedLSTMPredictor\n",
        "from loss import LossFunction\n",
        "from torch import optim\n",
        "\n",
        "DEVICE = 'cuda'\n",
        "model = ImprovedLSTMPredictor(\n",
        "    input_dim=13,\n",
        "    output_dim=5,\n",
        "    d_model=256,\n",
        "    hidden_dim=256,\n",
        "    num_encoder_layers=2,\n",
        "    num_decoder_layers=2,\n",
        "    dropout=0.1,\n",
        "    teacher_forcing_ratio=0.5,\n",
        ").to(DEVICE)\n",
        "criterion = LossFunction(loss1_coeff=0.5, loss2_coeff=0, loss3_coeff=0.5, loss4_coeff=0)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e071a4b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: load a pretrained LSTM checkpoint\n",
        "# model.load_weight('pretrained/lstm-improved.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e42a74ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.evaluate(val_loader, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3fcaf04",
      "metadata": {},
      "outputs": [],
      "source": [
        "LR = 1e-5\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    betas=(0.9, 0.999),\n",
        "    weight_decay=1e-4,\n",
        "    eps=1e-8\n",
        ")\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS + 1)\n",
        "best_val_loss = float(\"inf\")\n",
        "log_file = open('file_lstm.log', 'w')\n",
        "log_file.close()\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    train_loss = model.train_one_epoch(train_loader, optimizer, criterion, device=DEVICE)\n",
        "    val_loss = model.evaluate(val_loader, criterion, device=DEVICE)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        model.save_weight('pretrained/lstm-improved.pth')\n",
        "\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.8f}, Val Loss = {val_loss:.8f}, LR = {current_lr:.8f}\")\n",
        "    log_file = open('file_lstm.log', 'a')\n",
        "    log_file.write(f\"Epoch {epoch}: Train Loss = {train_loss:.8f}, Val Loss = {val_loss:.8f}, LR = {current_lr:.8f}\\n\")\n",
        "    log_file.close()\n",
        "\n",
        "print(\"Training complete. Best Val Loss:\", best_val_loss)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
