{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a164b7d",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c8832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import GTSequenceDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "SEQ_IN_LEN = 10\n",
    "SEQ_OUT_LEN = 10\n",
    "SEQ_TOTAL_LEN = 20\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "BASE_DIR = '../../Datasets/'\n",
    "train_dataset = GTSequenceDataset.from_roots([\n",
    "    f'{BASE_DIR}DanceTrack/train',\n",
    "    f'{BASE_DIR}MOT17/train',\n",
    "    f'{BASE_DIR}MOT20/train'\n",
    "], seq_in_len=SEQ_IN_LEN, seq_out_len=SEQ_OUT_LEN, seq_total_len=SEQ_TOTAL_LEN)\n",
    "\n",
    "val_dataset = GTSequenceDataset.from_roots([\n",
    "    f'{BASE_DIR}DanceTrack/val',\n",
    "    f'{BASE_DIR}MOT17/val',\n",
    "    f'{BASE_DIR}MOT20/val'\n",
    "], seq_in_len=SEQ_IN_LEN, seq_out_len=SEQ_OUT_LEN, seq_total_len=SEQ_TOTAL_LEN, noise_prob=0.6, noise_coeff=2)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f'Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572e9f25",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_encoder import MotionTransformer\n",
    "from loss import LossFunction\n",
    "from torch import optim\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "# model = MotionTransformer(num_enc_layers=1, num_dec_layers=1, dim_ff=64, d_model=32, dropout=0, nhead=4).to(DEVICE)\n",
    "model = MotionTransformer(d_model=128, dim_ff=256).to(DEVICE)\n",
    "# model = LSTMPredictor(middle_dim=64, hidden_dim=256, num_layers=1).to(DEVICE)\n",
    "criterion = LossFunction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbca6b8",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87955aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 2e-3\n",
    "NUM_EPOCHS = 40\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS + 1)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = model.train_one_epoch(train_loader, optimizer, criterion)\n",
    "    val_loss = model.evaluate(val_loader, criterion)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model.save_weight('pretrained/transformer-encoder-d128-ff256-3l.pth')\n",
    "\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.8f}, Val Loss = {val_loss:.8f}, LR = {current_lr:.8f}\")\n",
    "\n",
    "print(\"Training complete. Best Val Loss:\", best_val_loss)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
